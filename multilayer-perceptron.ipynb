{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed88363c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten, Dropout\n",
    "from tensorflow.keras.layers import GlobalMaxPooling2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import regularizers, optimizers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5ad5619",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset/iot23_01.csv', index_col=[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0869c226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conn_state_OTH</th>\n",
       "      <th>conn_state_REJ</th>\n",
       "      <th>conn_state_RSTO</th>\n",
       "      <th>conn_state_RSTOS0</th>\n",
       "      <th>conn_state_RSTR</th>\n",
       "      <th>conn_state_RSTRH</th>\n",
       "      <th>conn_state_S0</th>\n",
       "      <th>conn_state_S1</th>\n",
       "      <th>conn_state_S2</th>\n",
       "      <th>conn_state_S3</th>\n",
       "      <th>...</th>\n",
       "      <th>missed_bytes</th>\n",
       "      <th>orig_bytes</th>\n",
       "      <th>orig_ip_bytes</th>\n",
       "      <th>orig_pkts</th>\n",
       "      <th>proto_icmp</th>\n",
       "      <th>proto_tcp</th>\n",
       "      <th>proto_udp</th>\n",
       "      <th>resp_bytes</th>\n",
       "      <th>resp_ip_bytes</th>\n",
       "      <th>resp_pkts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4344</td>\n",
       "      <td>157</td>\n",
       "      <td>2381</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>54305</td>\n",
       "      <td>56393</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>10136</td>\n",
       "      <td>90</td>\n",
       "      <td>2302</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>54305</td>\n",
       "      <td>56393</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>180</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   conn_state_OTH  conn_state_REJ  conn_state_RSTO  conn_state_RSTOS0  \\\n",
       "0               0               0                0                  0   \n",
       "1               0               0                0                  0   \n",
       "2               0               0                0                  0   \n",
       "3               0               0                0                  0   \n",
       "4               0               0                0                  0   \n",
       "\n",
       "   conn_state_RSTR  conn_state_RSTRH  conn_state_S0  conn_state_S1  \\\n",
       "0                0                 0              1              0   \n",
       "1                0                 0              1              0   \n",
       "2                0                 0              0              0   \n",
       "3                0                 0              0              0   \n",
       "4                0                 0              1              0   \n",
       "\n",
       "   conn_state_S2  conn_state_S3  ...  missed_bytes  orig_bytes  orig_ip_bytes  \\\n",
       "0              0              0  ...             0           0             60   \n",
       "1              0              0  ...             0           0             60   \n",
       "2              0              0  ...          4344         157           2381   \n",
       "3              0              0  ...         10136          90           2302   \n",
       "4              0              0  ...             0           0            180   \n",
       "\n",
       "   orig_pkts  proto_icmp  proto_tcp  proto_udp  resp_bytes  resp_ip_bytes  \\\n",
       "0          1           0          1          0           0              0   \n",
       "1          1           0          1          0           0              0   \n",
       "2         41           0          1          0       54305          56393   \n",
       "3         41           0          1          0       54305          56393   \n",
       "4          3           0          1          0           0              0   \n",
       "\n",
       "   resp_pkts  \n",
       "0          0  \n",
       "1          0  \n",
       "2         40  \n",
       "3         40  \n",
       "4          0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8986ffbc",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m X \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(X)\n\u001b[1;32m      5\u001b[0m y \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 7\u001b[0m \u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "X = df.drop('label', axis=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5423b094",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0531dd97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.68305426\n",
      "Iteration 2, loss = 0.63219551\n",
      "Iteration 3, loss = 0.61660083\n",
      "Iteration 4, loss = 0.61174816\n",
      "Iteration 5, loss = 0.61063328\n",
      "Iteration 6, loss = 0.61046280\n",
      "Iteration 7, loss = 0.61044526\n",
      "Iteration 8, loss = 0.61043675\n",
      "Iteration 9, loss = 0.61045380\n",
      "Iteration 10, loss = 0.61044902\n",
      "Iteration 11, loss = 0.61044658\n",
      "Iteration 12, loss = 0.61044581\n",
      "Iteration 13, loss = 0.61044380\n",
      "Iteration 14, loss = 0.61044510\n",
      "Iteration 15, loss = 0.61044417\n",
      "Iteration 16, loss = 0.61043782\n",
      "Iteration 17, loss = 0.61044153\n",
      "Iteration 18, loss = 0.61044009\n",
      "Iteration 19, loss = 0.61043974\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(alpha=0.001, hidden_layer_sizes=(10, 5, 2), max_iter=1000,\n",
       "              random_state=21, tol=1e-09, verbose=10)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(10, 5, 2), max_iter=1000, alpha=0.001, solver='adam', verbose=10, random_state=21, tol=0.000000001)\n",
    "\n",
    "mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f55f7940",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4fbe608f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.69905\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "657d43b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp = MLPClassifier(hidden_layer_sizes=(115, 58), max_iter=1000, alpha=0.0001, solver='adam', verbose=10, random_state=21, tol=0.000000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "72c8ac1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.03562382\n",
      "Iteration 2, loss = 0.06223053\n",
      "Iteration 3, loss = 0.06546359\n",
      "Iteration 4, loss = 0.07412732\n",
      "Iteration 5, loss = 0.06367283\n",
      "Iteration 6, loss = 0.06957225\n",
      "Iteration 7, loss = 0.11980490\n",
      "Iteration 8, loss = 0.26025471\n",
      "Iteration 9, loss = 0.25653539\n",
      "Iteration 10, loss = 0.26570289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mlo lab\\.conda\\envs\\avadhoot\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:619: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(115, 58), max_iter=1000, random_state=21,\n",
       "              tol=1e-09, verbose=10)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5046cb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = mlp.predict(X_test)\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca30204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp = MLPClassifier(hidden_layer_sizes=(115, 58, 29), activation='elu', max_iter=1000, alpha=0.0001, solver='adam', verbose=10, random_state=21, tol=0.000000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0439a6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bf9d96aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "564ba002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7af8afd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00000000e+00 5.55111512e-17 0.00000000e+00 ... 0.00000000e+00\n",
      "  5.65430906e-01 3.72615015e-01]\n",
      " [0.00000000e+00 5.55111512e-17 0.00000000e+00 ... 0.00000000e+00\n",
      "  5.65430906e-01 3.72615015e-01]\n",
      " [0.00000000e+00 5.55111512e-17 0.00000000e+00 ... 0.00000000e+00\n",
      "  5.65430906e-01 3.72615015e-01]\n",
      " ...\n",
      " [2.17650233e-01 3.24239667e-01 3.89197457e-02 ... 0.00000000e+00\n",
      "  5.65430906e-01 3.72615015e-01]\n",
      " [4.28471972e-01 1.01140495e-02 4.17240671e-06 ... 0.00000000e+00\n",
      "  5.65430906e-01 3.72615015e-01]\n",
      " [2.41274952e-01 9.84552573e-03 7.78218863e-06 ... 0.00000000e+00\n",
      "  5.65430906e-01 3.72615015e-01]]\n"
     ]
    }
   ],
   "source": [
    "# print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bdff8bcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79999"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fe5f254a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f40f0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79999, 24)\n",
      "StratifiedKFold(n_splits=8, random_state=None, shuffle=True)\n",
      "Test Size = (10000,)\n",
      "Train size = (69999,)\n",
      "Fold 0:\n",
      "  Train: index=[    0     2     3 ... 79993 79995 79998]\n",
      "  Test:  index=[    1    14    18 ... 79994 79996 79997]\n",
      "Test Size = (10000,)\n",
      "Train size = (69999,)\n",
      "Fold 1:\n",
      "  Train: index=[    1     3     4 ... 79996 79997 79998]\n",
      "  Test:  index=[    0     2     6 ... 79956 79991 79992]\n",
      "Test Size = (10000,)\n",
      "Train size = (69999,)\n",
      "Fold 2:\n",
      "  Train: index=[    0     1     2 ... 79996 79997 79998]\n",
      "  Test:  index=[    5    22    26 ... 79973 79974 79979]\n",
      "Test Size = (10000,)\n",
      "Train size = (69999,)\n",
      "Fold 3:\n",
      "  Train: index=[    0     1     2 ... 79996 79997 79998]\n",
      "  Test:  index=[    7    24    28 ... 79978 79985 79986]\n",
      "Test Size = (10000,)\n",
      "Train size = (69999,)\n",
      "Fold 4:\n",
      "  Train: index=[    0     1     2 ... 79996 79997 79998]\n",
      "  Test:  index=[    3     4    12 ... 79964 79968 79995]\n",
      "Test Size = (10000,)\n",
      "Train size = (69999,)\n",
      "Fold 5:\n",
      "  Train: index=[    0     1     2 ... 79995 79996 79997]\n",
      "  Test:  index=[   11    15    19 ... 79982 79990 79998]\n",
      "Test Size = (10000,)\n",
      "Train size = (69999,)\n",
      "Fold 6:\n",
      "  Train: index=[    0     1     2 ... 79996 79997 79998]\n",
      "  Test:  index=[   10    16    52 ... 79988 79989 79993]\n",
      "Test Size = (9999,)\n",
      "Train size = (70000,)\n",
      "Fold 7:\n",
      "  Train: index=[    0     1     2 ... 79996 79997 79998]\n",
      "  Test:  index=[    8     9    17 ... 79958 79960 79962]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pandas as pd\n",
    "print(X_train.shape)\n",
    "skf = StratifiedKFold(n_splits=8,shuffle=True)\n",
    "skf.get_n_splits(X_train, y_train)\n",
    "print(skf)\n",
    "splits = skf.split(np.zeros(len(X_train)), y_train)\n",
    "for i, (train_index, test_index) in enumerate(skf.split(np.zeros(len(X_train)), y_train)):\n",
    "    #print(pd.DataFrame(train_index).describe())\n",
    "    print(f\"Test Size = {test_index.shape}\")\n",
    "    print(f\"Train size = {train_index.shape}\")\n",
    "    print(f\"Fold {i}:\")\n",
    "    print(f\"  Train: index={train_index}\")\n",
    "    print(f\"  Test:  index={test_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1a358d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'[3, 17, 36, 52, 54, 105, 193, 213, 248, 276, 281, 302, 328, 368, 382, 489, 526, 548, 590, 660, 692, 858, 880, 910, 926, 971, 988, 1083, 1183, 1194, 1202, 1240, 1269, 1300, 1309, 1312, 1457, 1514, 1540, 1592, 1648, 1704, 1771, 1844, 1872, 1954, 1984, 2034, 2053, 2060, 2065, 2091, 2181, 2196, 2288, 2313, 2324, 2485, 2491, 2499, 2503, 2525, 2530, 2574, 2576, 2633, 2636, 2637, 2730, 2753, 2773, 2848, 2857, 2866, 2883, 2919, 2950, 2961, 3051, 3057, 3088, 3152, 3154, 3160, 3241, 3261, 3265, 3294, 3427, 3462, 3468, 3518, 3536, 3546, 3550, 3564, 3715, 3718, 3772, 3849, 3885, 3905, 3948, 4246, 4250, 4279, 4320, 4414, 4419, 4432, 4438, 4504, 4541, 4604, 4677, 4688, 4751, 4794, 4796, 4837, 4894, 4922, 4943, 4948, 4982, 5012, 5014, 5029, 5034, 5047, 5056, 5070, 5088, 5095, 5107, 5203, 5230, 5302, 5338, 5358, 5379, 5399, 5413, 5418, 5511, 5517, 5522, 5528, 5551, 5624, 5710, 5716, 5735, 5752, 5794, 5803, 5868, 5901, 5914, 5976, 6038, 6069, 6092, 6147, 6224, 6289, 6295, 6345, 6423, 6596, 6607, 6715, 6732, 6736, 6770, 6783, 6785, 6887, 6947, 7017, 7020, 7078, 7126, 7164, 7169, 7170, 7185, 7243, 7310, 7502, 7514, 7593, 7642, 7659, 7676, 7755, 7770, 7776, 7891, 7895, 7957, 8013, 8032, 8096, 8127, 8231, 8317, 8326, 8329, 8415, 8482, 8486, 8638, 8674, 8700, 8746, 8786, 8849, 8854, 8893, 8908, 8924, 9006, 9045, 9069, 9085, 9135, 9145, 9154, 9192, 9255, 9298, 9314, 9343, 9370, 9393, 9410, 9482, 9484, 9565, 9652, 9673, 9679, 9687, 9739, 9745, 9860, 9914, 9949, 9983, 10029, 10101, 10109, 10115, 10122, 10124, 10154, 10200, 10229, 10273, 10281, 10327, 10429, 10440, 10469, 10560, 10572, 10590, 10606, 10622, 10624, 10662, 10685, 10698, 10699, 10739, 10758, 10763, 10858, 10862, 10940, 10995, 11025, 11044, 11064, 11097, 11124, 11160, 11199, 11252, 11364, 11399, 11430, 11453, 11512, 11538, 11610, 11660, 11670, 11694, 11698, 11714, 11745, 11774, 11850, 11898, 11991, 12034, 12043, 12077, 12124, 12172, 12255, 12259, 12270, 12324, 12362, 12382, 12385, 12401, 12427, 12477, 12491, 12505, 12518, 12528, 12530, 12559, 12598, 12634, 12678, 12679, 12718, 12723, 12763, 12788, 12796, 12807, 12832, 12845, 12942, 12943, 12948, 13006, 13034, 13045, 13138, 13144, 13335, 13339, 13365, 13444, 13517, 13588, 13622, 13639, 13703, 13716, 13721, 13754, 13849, 13912, 13962, 14071, 14192, 14292, 14319, 14320, 14350, 14390, 14391, 14626, 14642, 14644, 14708, 14849, 14850, 14894, 14939, 14965, 15054, 15060, 15124, 15209, 15232, 15260, 15264, 15268, 15307, 15309, 15339, 15342, 15377, 15448, 15474, 15531, 15567, 15690, 15722, 15801, 15822, 15868, 15911, 16004, 16097, 16110, 16181, 16221, 16241, 16277, 16279, 16303, 16391, 16447, 16479, 16525, 16555, 16588, 16636, 16656, 16683, 16737, 16750, 16769, 16867, 16930, 16945, 16951, 16953, 16966, 16976, 16990, 17097, 17206, 17217, 17232, 17247, 17307, 17376, 17399, 17474, 17489, 17490, 17491, 17510, 17545, 17590, 17595, 17639, 17678, 17722, 17733, 17837, 17851, 17858, 17860, 17872, 17892, 17912, 17913, 17928, 18039, 18056, 18086, 18101, 18145, 18228, 18229, 18414, 18431, 18477, 18502, 18547, 18604, 18623, 18630, 18717, 18733, 18768, 18801, 18816, 18817, 18873, 18941, 19107, 19222, 19250, 19268, 19271, 19307, 19330, 19392, 19421, 19443, 19493, 19520, 19529, 19593, 19597, 19628, 19658, 19659, 19695, 19802, 19821, 19828, 19883, 19944, 19972, 19998, 20017, 20097, 20134, 20150, 20189, 20216, 20280, 20316, 20336, 20384, 20386, 20436, 20500, 20520, 20538, 20562, 20591, 20593, 20603, 20627, 20629, 20668, 20698, 20736, 20779, 20786, 20821, 20822, 20885, 20894, 20900, 20921, 20930, 20979, 21013, 21022, 21045, 21060, 21131, 21216, 21250, 21317, 21441, 21570, 21602, 21683, 21691, 21761, 21805, 21806, 21854, 21872, 21876, 22122, 22156, 22471, 22479, 22481, 22504, 22518, 22546, 22580, 22649, 22737, 22741, 22808, 22889, 22928, 22942, 22950, 23033, 23052, 23266, 23284, 23293, 23335, 23364, 23510, 23546, 23557, 23564, 23656, 23682, 23746, 23870, 23973, 24037, 24057, 24063, 24088, 24094, 24128, 24165, 24185, 24191, 24228, 24274, 24284, 24296, 24353, 24375, 24430, 24466, 24499, 24628, 24695, 24705, 24711, 24753, 24778, 24819, 24842, 24902, 24910, 24930, 25016, 25138, 25243, 25252, 25283, 25662, 25678, 25714, 25715, 25772, 25800, 25864, 25865, 25897, 25911, 25920, 25941, 25942, 26022, 26030, 26050, 26065, 26075, 26086, 26121, 26122, 26141, 26155, 26166, 26243, 26312, 26391, 26455, 26476, 26495, 26507, 26554, 26557, 26574, 26588, 26645, 26689, 26820, 26828, 26853, 26905, 26988, 27073, 27076, 27159, 27191, 27238, 27248, 27265, 27431, 27452, 27475, 27560, 27588, 27594, 27706, 27731, 27797, 27805, 27814, 27879, 27893, 27976, 27994, 27996, 28015, 28055, 28115, 28152, 28178, 28220, 28251, 28332, 28397, 28469, 28476, 28508, 28556, 28606, 28730, 28782, 28785, 28809, 28910, 28964, 28987, 29058, 29063, 29081, 29085, 29086, 29100, 29115, 29189, 29193, 29202, 29205, 29220, 29258, 29262, 29318, 29343, 29449, 29479, 29513, 29529, 29532, 29565, 29575, 29614, 29631, 29756, 29766, 29815, 29829, 29865, 30137, 30177, 30192, 30241, 30277, 30285, 30321, 30364, 30404, 30417, 30420, 30424, 30426, 30449, 30465, 30471, 30510, 30553, 30575, 30632, 30749, 30777, 30895, 30898, 30945, 30952, 31021, 31094, 31113, 31177, 31185, 31209, 31251, 31268, 31328, 31335, 31337, 31348, 31350, 31382, 31521, 31565, 31701, 31736, 31752, 31774, 31796, 31895, 31933, 32057, 32082, 32095, 32142, 32200, 32222, 32224, 32252, 32253, 32306, 32333, 32342, 32358, 32380, 32386, 32409, 32460, 32582, 32712, 32729, 32761, 32766, 32890, 32915, 32919, 32991, 32996, 33093, 33125, 33134, 33146, 33150, 33169, 33174, 33194, 33207, 33222, 33377, 33380, 33454, 33478, 33486, 33500, 33526, 33535, 33543, 33626, 33664, 33674, 33680, 33703, 33728, 33741, 33771, 33781, 33816, 33846, 33887, 33904, 33982, 33984, 34008, 34012, 34013, 34016, 34019, 34098, 34145, 34249, 34279, 34318, 34362, 34388, 34403, 34439, 34442, 34494, 34505, 34572, 34601, 34614, 34621, 34622, 34762, 34811, 34840, 34843, 34900, 34914, 34951, 35027, 35028, 35100, 35142, 35165, 35178, 35182, 35209, 35240, 35241, 35266, 35403, 35414, 35432, 35462, 35494, 35503, 35544, 35559, 35576, 35690, 35749, 35774, 35777, 35870, 35873, 35909, 35940, 36122, 36203, 36206, 36242, 36246, 36287, 36305, 36331, 36370, 36371, 36379, 36401, 36512, 36518, 36534, 36545, 36595, 36755, 36763, 36838, 36843, 36867, 36979, 37078, 37127, 37153, 37158, 37244, 37298, 37317, 37413, 37442, 37463, 37509, 37537, 37602, 37620, 37630, 37632, 37643, 37658, 37706, 37712, 37750, 37770, 37863, 37908, 37909, 38061, 38064, 38095, 38106, 38167, 38265, 38308, 38344, 38347, 38352, 38473, 38497, 38557, 38638, 38640, 38675, 38701, 38725, 38735, 38755, 38843, 38866, 38905, 38938, 38999, 39035, 39065, 39085, 39138, 39150, 39229, 39258, 39383, 39393, 39415, 39511, 39528, 39601, 39647, 39658, 39692, 39846, 40002, 40092, 40100, 40115, 40136, 40233, 40276, 40288, 40301, 40305, 40306, 40427, 40545, 40564, 40707, 40736, 40756, 40769, 40784, 40804, 40814, 40828, 40865, 40887, 40932, 40941, 40944, 40959, 40978, 41032, 41049, 41053, 41106, 41123, 41135, 41150, 41157, 41173, 41240, 41249, 41325, 41379, 41392, 41468, 41534, 41596, 41653, 41788, 41841, 41846, 41905, 41922, 41933, 41966, 41998, 42017, 42054, 42072, 42135, 42157, 42182, 42193, 42212, 42224, 42238, 42250, 42282, 42347, 42382, 42418, 42436, 42599, 42635, 42682, 42754, 42789, 42817, 42856, 42913, 42932, 42981, 43043, 43070, 43097, 43124, 43199, 43280, 43291, 43319, 43323, 43336, 43356, 43482, 43550, 43577, 43595, 43620, 43659, 43725, 43736, 43765, 43766, 43803, 43842, 43975, 43989, 43999, 44013, 44100, 44103, 44129, 44165, 44203, 44253, 44254, 44258, 44322, 44383, 44433, 44460, 44468, 44558, 44580, 44657, 44812, 44819, 44853, 44882, 44884, 44903, 44993, 44996, 45071, 45096, 45138, 45175, 45188, 45210, 45221, 45270, 45280, 45322, 45351, 45420, 45428, 45583, 45594, 45596, 45621, 45669, 45722, 45769, 45772, 45820, 45861, 45876, 45883, 45949, 45972, 45996, 46004, 46011, 46059, 46090, 46126, 46268, 46271, 46282, 46328, 46366, 46452, 46490, 46535, 46552, 46593, 46663, 46693, 46695, 46696, 46739, 46749, 46767, 46773, 46777, 46837, 46860, 46887, 46922, 46946, 46949, 46967, 47011, 47048, 47051, 47065, 47127, 47156, 47167, 47253, 47285, 47299, 47349, 47410, 47414, 47421, 47431, 47540, 47550, 47554, 47559, 47592, 47607, 47636, 47677, 47687, 47713, 47727, 47851, 47852, 47855, 47856, 47958, 47980, 48007, 48015, 48028, 48116, 48130, 48145, 48154, 48158, 48183, 48198, 48244, 48266, 48357, 48414, 48452, 48463, 48471, 48523, 48567, 48679, 48710, 48769, 48773, 48830, 48836, 48845, 48938, 48954, 49030, 49051, 49085, 49089, 49145, 49368, 49384, 49527, 49556, 49562, 49592, 49601, 49657, 49676, 49698, 49706, 49733, 49800, 49802, 49803, 49816, 49829, 49876, 49903, 49930, 49984, 50047, 50094, 50137, 50167, 50180, 50214, 50219, 50223, 50275, 50462, 50514, 50593, 50606, 50727, 50809, 50829, 50833, 50850, 50984, 51091, 51121, 51130, 51167, 51206, 51238, 51263, 51340, 51406, 51411, 51491, 51504, 51508, 51626, 51669, 51744, 51769, 51815, 52049, 52051, 52064, 52068, 52089, 52157, 52189, 52211, 52263, 52352, 52384, 52406, 52563, 52583, 52599, 52894, 52964, 53003, 53124, 53132, 53265, 53352, 53396, 53558, 53565, 53566, 53588, 53693, 53742, 53800, 53828, 53845, 53902, 53953, 54004, 54025, 54081, 54111, 54167, 54232, 54250, 54251, 54290, 54316, 54349, 54366, 54383, 54384, 54386, 54395, 54428, 54499, 54547, 54551, 54680, 54716, 54745, 54748, 54751, 54785, 54791, 54820, 54828, 54830, 54859, 54860, 54879, 54960, 55032, 55063, 55122, 55190, 55222, 55245, 55249, 55261, 55262, 55263, 55288, 55295, 55322, 55365, 55373, 55415, 55552, 55566, 55618, 55630, 55656, 55665, 55683, 55717, 55757, 55814, 55860, 55929, 55960, 55977, 55993, 56088, 56089, 56163, 56178, 56190, 56241, 56245, 56246, 56279, 56309, 56318, 56319, 56401, 56450, 56458, 56500, 56540, 56564, 56571, 56649, 56700, 56849, 56949, 56958, 56970, 56988, 57047, 57049, 57070, 57153, 57156, 57161, 57294, 57296, 57320, 57323, 57349, 57377, 57485, 57512, 57541, 57637, 57690, 57788, 57813, 57836, 57881, 57908, 57909, 57916, 57937, 57980, 58088, 58126, 58137, 58156, 58169, 58174, 58209, 58217, 58219, 58249, 58259, 58277, 58342, 58353, 58360, 58364, 58380, 58383, 58412, 58484, 58495, 58613, 58656, 58676, 58683, 58738, 58740, 58745, 58768, 58818, 58844, 58849, 58891, 58952, 58991, 59006, 59009, 59038, 59046, 59057, 59062, 59096, 59106, 59195, 59231, 59291, 59306, 59312, 59324, 59449, 59502, 59522, 59540, 59616, 59620, 59671, 59818, 59834, 59930, 59933, 59985, 60055, 60078, 60273, 60348, 60356, 60361, 60446, 60559, 60601, 60639, 60685, 60688, 60694, 60799, 60801, 60824, 60962, 61040, 61061, 61077, 61098, 61242, 61277, 61314, 61322, 61325, 61346, 61354, 61393, 61459, 61473, 61545, 61575, 61612, 61615, 61616, 61642, 61652, 61655, 61657, 61685, 61703, 61747, 61852, 61867, 61878, 61899, 61938, 61967, 61989, 62056, 62128, 62131, 62140, 62143, 62154, 62176, 62177, 62201, 62246, 62282, 62332, 62351, 62375, 62387, 62389, 62401, 62434, 62465, 62482, 62523, 62532, 62557, 62578, 62616, 62625, 62661, 62683, 62738, 62817, 62868, 62880, 62902, 62935, 62939, 63167, 63273, 63321, 63334, 63430, 63454, 63459, 63489, 63515, 63519, 63528, 63533, 63557, 63595, 63611, 63620, 63628, 63651, 63692, 63754, 63785, 63802, 63931, 63954, 64019, 64040, 64058, 64133, 64186, 64217, 64365, 64369, 64388, 64420, 64461, 64506, 64523, 64611, 64778, 64844, 64900, 64902, 64975, 64992, 65122, 65163, 65182, 65207, 65220, 65242, 65244, 65293, 65338, 65352, 65384, 65404, 65424, 65497, 65545, 65563, 65666, 65730, 65741, 65845, 65881, 65915, 65918, 65926, 65932, 65947, 66048, 66138, 66188, 66207, 66208, 66255, 66295, 66296, 66311, 66368, 66433, 66469, 66489, 66545, 66592, 66594, 66600, 66605, 66616, 66633, 66676, 66734, 66736, 66746, 66777, 66782, 66837, 66861, 66929, 67000, 67047, 67084, 67121, 67131, 67153, 67159, 67182, 67225, 67323, 67357, 67422, 67425, 67479, 67498, 67500, 67572, 67578, 67605, 67644, 67660, 67716, 67737, 67768, 67772, 67821, 67861, 67897, 67920, 67951, 67968, 67978, 67984, 68006, 68043, 68141, 68220, 68325, 68331, 68334, 68371, 68386, 68428, 68484, 68504, 68552, 68564, 68641, 68663, 68711, 68808, 68831, 68855, 68869, 69052, 69061, 69083, 69139, 69170, 69198, 69228, 69239, 69307, 69330, 69394, 69408, 69435, 69487, 69545, 69575, 69620, 69660, 69663, 69690, 69702, 69738, 69748, 69756, 69762, 69766, 69779, 69931, 69989, 70117, 70139, 70145, 70168, 70230, 70234, 70294, 70318, 70363, 70569, 70589, 70644, 70715, 70745, 70747, 70900, 71010, 71030, 71047, 71053, 71068, 71074, 71085, 71103, 71119, 71131, 71179, 71199, 71225, 71330, 71335, 71340, 71429, 71450, 71518, 71548, 71558, 71567, 71616, 71641, 71669, 71687, 71715, 71754, 71784, 71806, 71824, 71833, 71834, 71882, 71883, 71889, 71941, 71951, 71959, 71963, 71967, 71985, 71992, 72093, 72109, 72110, 72138, 72324, 72340, 72385, 72454, 72467, 72476, 72490, 72569, 72632, 72639, 72880, 72886, 72890, 72898, 72956, 73042, 73082, 73212, 73238, 73253, 73270, 73328, 73378, 73389, 73451, 73494, 73517, 73519, 73589, 73615, 73634, 73757, 73761, 73773, 73844, 73863, 73959, 74018, 74040, 74046, 74075, 74086, 74098, 74149, 74157, 74172, 74181, 74218, 74301, 74302, 74310, 74336, 74464, 74486, 74502, 74517, 74580, 74581, 74585, 74591, 74595, 74599, 74605, 74609, 74614, 74625, 74665, 74683, 74695, 74739, 74783, 74817, 74832, 74876, 74935, 74937, 74967, 74982, 74989, 74990, 75171, 75247, 75252, 75326, 75330, 75353, 75431, 75507, 75607, 75633, 75717, 75726, 75732, 75808, 75847, 75895, 75907, 75915, 75920, 75935, 75940, 75964, 75979, 76066, 76074, 76120, 76248, 76265, 76274, 76355, 76397, 76518, 76535, 76561, 76566, 76606, 76635, 76662, 76783, 76827, 76828, 76834, 76860, 76886, 76891, 77002, 77096, 77170, 77194, 77199, 77250, 77268, 77345, 77515, 77553, 77634, 77644, 77657, 77692, 77753, 77755, 77758, 77759, 77796, 77814, 77817, 77825, 77857, 77864, 77965, 78010, 78072, 78078, 78142, 78184, 78194, 78219, 78356, 78372, 78393, 78399, 78439, 78447, 78450, 78474, 78494, 78585, 78669, 78728, 78796, 78833, 78929, 78932, 78964, 78998, 79018, 79143, 79165, 79183, 79192, 79321, 79326, 79335, 79349, 79410, 79437, 79441, 79452, 79468, 79520, 79602, 79622, 79639, 79646, 79715, 79727, 79817, 79849, 79871, 79916, 79920] not in index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;28mlen\u001b[39m(X_train), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[1;32m      7\u001b[0m mask[b,] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m x, y \u001b[38;5;241m=\u001b[39m X_train[b], \u001b[43my_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mb\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;66;03m# instead of a[b] you could also do a[~mask]\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#print(len(x))\u001b[39;00m\n\u001b[1;32m     10\u001b[0m X_trains\u001b[38;5;241m.\u001b[39mappend(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/series.py:984\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    981\u001b[0m     key \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(key, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[1;32m    982\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_values(key)\n\u001b[0;32m--> 984\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_with\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/series.py:1019\u001b[0m, in \u001b[0;36mSeries._get_with\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minteger\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;66;03m# We need to decide whether to treat this as a positional indexer\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m     \u001b[38;5;66;03m#  (i.e. self.iloc) or label-based (i.e. self.loc)\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39m_should_fallback_to_positional:\n\u001b[0;32m-> 1019\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1020\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1021\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miloc[key]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexing.py:967\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    964\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    966\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[0;32m--> 967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexing.py:1191\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1188\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index with multidimensional key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_iterable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[38;5;66;03m# nested tuple slicing\u001b[39;00m\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_nested_tuple(key, labels):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexing.py:1132\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_iterable\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_key(key, axis)\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# A collection of keys\u001b[39;00m\n\u001b[0;32m-> 1132\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_listlike_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_reindex_with_indexers(\n\u001b[1;32m   1134\u001b[0m     {axis: [keyarr, indexer]}, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_dups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexing.py:1327\u001b[0m, in \u001b[0;36m_LocIndexer._get_listlike_indexer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1324\u001b[0m ax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis)\n\u001b[1;32m   1325\u001b[0m axis_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis_name(axis)\n\u001b[0;32m-> 1327\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m keyarr, indexer\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:5782\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   5779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5780\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 5782\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5784\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   5785\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   5786\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:5845\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   5842\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   5844\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 5845\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: '[3, 17, 36, 52, 54, 105, 193, 213, 248, 276, 281, 302, 328, 368, 382, 489, 526, 548, 590, 660, 692, 858, 880, 910, 926, 971, 988, 1083, 1183, 1194, 1202, 1240, 1269, 1300, 1309, 1312, 1457, 1514, 1540, 1592, 1648, 1704, 1771, 1844, 1872, 1954, 1984, 2034, 2053, 2060, 2065, 2091, 2181, 2196, 2288, 2313, 2324, 2485, 2491, 2499, 2503, 2525, 2530, 2574, 2576, 2633, 2636, 2637, 2730, 2753, 2773, 2848, 2857, 2866, 2883, 2919, 2950, 2961, 3051, 3057, 3088, 3152, 3154, 3160, 3241, 3261, 3265, 3294, 3427, 3462, 3468, 3518, 3536, 3546, 3550, 3564, 3715, 3718, 3772, 3849, 3885, 3905, 3948, 4246, 4250, 4279, 4320, 4414, 4419, 4432, 4438, 4504, 4541, 4604, 4677, 4688, 4751, 4794, 4796, 4837, 4894, 4922, 4943, 4948, 4982, 5012, 5014, 5029, 5034, 5047, 5056, 5070, 5088, 5095, 5107, 5203, 5230, 5302, 5338, 5358, 5379, 5399, 5413, 5418, 5511, 5517, 5522, 5528, 5551, 5624, 5710, 5716, 5735, 5752, 5794, 5803, 5868, 5901, 5914, 5976, 6038, 6069, 6092, 6147, 6224, 6289, 6295, 6345, 6423, 6596, 6607, 6715, 6732, 6736, 6770, 6783, 6785, 6887, 6947, 7017, 7020, 7078, 7126, 7164, 7169, 7170, 7185, 7243, 7310, 7502, 7514, 7593, 7642, 7659, 7676, 7755, 7770, 7776, 7891, 7895, 7957, 8013, 8032, 8096, 8127, 8231, 8317, 8326, 8329, 8415, 8482, 8486, 8638, 8674, 8700, 8746, 8786, 8849, 8854, 8893, 8908, 8924, 9006, 9045, 9069, 9085, 9135, 9145, 9154, 9192, 9255, 9298, 9314, 9343, 9370, 9393, 9410, 9482, 9484, 9565, 9652, 9673, 9679, 9687, 9739, 9745, 9860, 9914, 9949, 9983, 10029, 10101, 10109, 10115, 10122, 10124, 10154, 10200, 10229, 10273, 10281, 10327, 10429, 10440, 10469, 10560, 10572, 10590, 10606, 10622, 10624, 10662, 10685, 10698, 10699, 10739, 10758, 10763, 10858, 10862, 10940, 10995, 11025, 11044, 11064, 11097, 11124, 11160, 11199, 11252, 11364, 11399, 11430, 11453, 11512, 11538, 11610, 11660, 11670, 11694, 11698, 11714, 11745, 11774, 11850, 11898, 11991, 12034, 12043, 12077, 12124, 12172, 12255, 12259, 12270, 12324, 12362, 12382, 12385, 12401, 12427, 12477, 12491, 12505, 12518, 12528, 12530, 12559, 12598, 12634, 12678, 12679, 12718, 12723, 12763, 12788, 12796, 12807, 12832, 12845, 12942, 12943, 12948, 13006, 13034, 13045, 13138, 13144, 13335, 13339, 13365, 13444, 13517, 13588, 13622, 13639, 13703, 13716, 13721, 13754, 13849, 13912, 13962, 14071, 14192, 14292, 14319, 14320, 14350, 14390, 14391, 14626, 14642, 14644, 14708, 14849, 14850, 14894, 14939, 14965, 15054, 15060, 15124, 15209, 15232, 15260, 15264, 15268, 15307, 15309, 15339, 15342, 15377, 15448, 15474, 15531, 15567, 15690, 15722, 15801, 15822, 15868, 15911, 16004, 16097, 16110, 16181, 16221, 16241, 16277, 16279, 16303, 16391, 16447, 16479, 16525, 16555, 16588, 16636, 16656, 16683, 16737, 16750, 16769, 16867, 16930, 16945, 16951, 16953, 16966, 16976, 16990, 17097, 17206, 17217, 17232, 17247, 17307, 17376, 17399, 17474, 17489, 17490, 17491, 17510, 17545, 17590, 17595, 17639, 17678, 17722, 17733, 17837, 17851, 17858, 17860, 17872, 17892, 17912, 17913, 17928, 18039, 18056, 18086, 18101, 18145, 18228, 18229, 18414, 18431, 18477, 18502, 18547, 18604, 18623, 18630, 18717, 18733, 18768, 18801, 18816, 18817, 18873, 18941, 19107, 19222, 19250, 19268, 19271, 19307, 19330, 19392, 19421, 19443, 19493, 19520, 19529, 19593, 19597, 19628, 19658, 19659, 19695, 19802, 19821, 19828, 19883, 19944, 19972, 19998, 20017, 20097, 20134, 20150, 20189, 20216, 20280, 20316, 20336, 20384, 20386, 20436, 20500, 20520, 20538, 20562, 20591, 20593, 20603, 20627, 20629, 20668, 20698, 20736, 20779, 20786, 20821, 20822, 20885, 20894, 20900, 20921, 20930, 20979, 21013, 21022, 21045, 21060, 21131, 21216, 21250, 21317, 21441, 21570, 21602, 21683, 21691, 21761, 21805, 21806, 21854, 21872, 21876, 22122, 22156, 22471, 22479, 22481, 22504, 22518, 22546, 22580, 22649, 22737, 22741, 22808, 22889, 22928, 22942, 22950, 23033, 23052, 23266, 23284, 23293, 23335, 23364, 23510, 23546, 23557, 23564, 23656, 23682, 23746, 23870, 23973, 24037, 24057, 24063, 24088, 24094, 24128, 24165, 24185, 24191, 24228, 24274, 24284, 24296, 24353, 24375, 24430, 24466, 24499, 24628, 24695, 24705, 24711, 24753, 24778, 24819, 24842, 24902, 24910, 24930, 25016, 25138, 25243, 25252, 25283, 25662, 25678, 25714, 25715, 25772, 25800, 25864, 25865, 25897, 25911, 25920, 25941, 25942, 26022, 26030, 26050, 26065, 26075, 26086, 26121, 26122, 26141, 26155, 26166, 26243, 26312, 26391, 26455, 26476, 26495, 26507, 26554, 26557, 26574, 26588, 26645, 26689, 26820, 26828, 26853, 26905, 26988, 27073, 27076, 27159, 27191, 27238, 27248, 27265, 27431, 27452, 27475, 27560, 27588, 27594, 27706, 27731, 27797, 27805, 27814, 27879, 27893, 27976, 27994, 27996, 28015, 28055, 28115, 28152, 28178, 28220, 28251, 28332, 28397, 28469, 28476, 28508, 28556, 28606, 28730, 28782, 28785, 28809, 28910, 28964, 28987, 29058, 29063, 29081, 29085, 29086, 29100, 29115, 29189, 29193, 29202, 29205, 29220, 29258, 29262, 29318, 29343, 29449, 29479, 29513, 29529, 29532, 29565, 29575, 29614, 29631, 29756, 29766, 29815, 29829, 29865, 30137, 30177, 30192, 30241, 30277, 30285, 30321, 30364, 30404, 30417, 30420, 30424, 30426, 30449, 30465, 30471, 30510, 30553, 30575, 30632, 30749, 30777, 30895, 30898, 30945, 30952, 31021, 31094, 31113, 31177, 31185, 31209, 31251, 31268, 31328, 31335, 31337, 31348, 31350, 31382, 31521, 31565, 31701, 31736, 31752, 31774, 31796, 31895, 31933, 32057, 32082, 32095, 32142, 32200, 32222, 32224, 32252, 32253, 32306, 32333, 32342, 32358, 32380, 32386, 32409, 32460, 32582, 32712, 32729, 32761, 32766, 32890, 32915, 32919, 32991, 32996, 33093, 33125, 33134, 33146, 33150, 33169, 33174, 33194, 33207, 33222, 33377, 33380, 33454, 33478, 33486, 33500, 33526, 33535, 33543, 33626, 33664, 33674, 33680, 33703, 33728, 33741, 33771, 33781, 33816, 33846, 33887, 33904, 33982, 33984, 34008, 34012, 34013, 34016, 34019, 34098, 34145, 34249, 34279, 34318, 34362, 34388, 34403, 34439, 34442, 34494, 34505, 34572, 34601, 34614, 34621, 34622, 34762, 34811, 34840, 34843, 34900, 34914, 34951, 35027, 35028, 35100, 35142, 35165, 35178, 35182, 35209, 35240, 35241, 35266, 35403, 35414, 35432, 35462, 35494, 35503, 35544, 35559, 35576, 35690, 35749, 35774, 35777, 35870, 35873, 35909, 35940, 36122, 36203, 36206, 36242, 36246, 36287, 36305, 36331, 36370, 36371, 36379, 36401, 36512, 36518, 36534, 36545, 36595, 36755, 36763, 36838, 36843, 36867, 36979, 37078, 37127, 37153, 37158, 37244, 37298, 37317, 37413, 37442, 37463, 37509, 37537, 37602, 37620, 37630, 37632, 37643, 37658, 37706, 37712, 37750, 37770, 37863, 37908, 37909, 38061, 38064, 38095, 38106, 38167, 38265, 38308, 38344, 38347, 38352, 38473, 38497, 38557, 38638, 38640, 38675, 38701, 38725, 38735, 38755, 38843, 38866, 38905, 38938, 38999, 39035, 39065, 39085, 39138, 39150, 39229, 39258, 39383, 39393, 39415, 39511, 39528, 39601, 39647, 39658, 39692, 39846, 40002, 40092, 40100, 40115, 40136, 40233, 40276, 40288, 40301, 40305, 40306, 40427, 40545, 40564, 40707, 40736, 40756, 40769, 40784, 40804, 40814, 40828, 40865, 40887, 40932, 40941, 40944, 40959, 40978, 41032, 41049, 41053, 41106, 41123, 41135, 41150, 41157, 41173, 41240, 41249, 41325, 41379, 41392, 41468, 41534, 41596, 41653, 41788, 41841, 41846, 41905, 41922, 41933, 41966, 41998, 42017, 42054, 42072, 42135, 42157, 42182, 42193, 42212, 42224, 42238, 42250, 42282, 42347, 42382, 42418, 42436, 42599, 42635, 42682, 42754, 42789, 42817, 42856, 42913, 42932, 42981, 43043, 43070, 43097, 43124, 43199, 43280, 43291, 43319, 43323, 43336, 43356, 43482, 43550, 43577, 43595, 43620, 43659, 43725, 43736, 43765, 43766, 43803, 43842, 43975, 43989, 43999, 44013, 44100, 44103, 44129, 44165, 44203, 44253, 44254, 44258, 44322, 44383, 44433, 44460, 44468, 44558, 44580, 44657, 44812, 44819, 44853, 44882, 44884, 44903, 44993, 44996, 45071, 45096, 45138, 45175, 45188, 45210, 45221, 45270, 45280, 45322, 45351, 45420, 45428, 45583, 45594, 45596, 45621, 45669, 45722, 45769, 45772, 45820, 45861, 45876, 45883, 45949, 45972, 45996, 46004, 46011, 46059, 46090, 46126, 46268, 46271, 46282, 46328, 46366, 46452, 46490, 46535, 46552, 46593, 46663, 46693, 46695, 46696, 46739, 46749, 46767, 46773, 46777, 46837, 46860, 46887, 46922, 46946, 46949, 46967, 47011, 47048, 47051, 47065, 47127, 47156, 47167, 47253, 47285, 47299, 47349, 47410, 47414, 47421, 47431, 47540, 47550, 47554, 47559, 47592, 47607, 47636, 47677, 47687, 47713, 47727, 47851, 47852, 47855, 47856, 47958, 47980, 48007, 48015, 48028, 48116, 48130, 48145, 48154, 48158, 48183, 48198, 48244, 48266, 48357, 48414, 48452, 48463, 48471, 48523, 48567, 48679, 48710, 48769, 48773, 48830, 48836, 48845, 48938, 48954, 49030, 49051, 49085, 49089, 49145, 49368, 49384, 49527, 49556, 49562, 49592, 49601, 49657, 49676, 49698, 49706, 49733, 49800, 49802, 49803, 49816, 49829, 49876, 49903, 49930, 49984, 50047, 50094, 50137, 50167, 50180, 50214, 50219, 50223, 50275, 50462, 50514, 50593, 50606, 50727, 50809, 50829, 50833, 50850, 50984, 51091, 51121, 51130, 51167, 51206, 51238, 51263, 51340, 51406, 51411, 51491, 51504, 51508, 51626, 51669, 51744, 51769, 51815, 52049, 52051, 52064, 52068, 52089, 52157, 52189, 52211, 52263, 52352, 52384, 52406, 52563, 52583, 52599, 52894, 52964, 53003, 53124, 53132, 53265, 53352, 53396, 53558, 53565, 53566, 53588, 53693, 53742, 53800, 53828, 53845, 53902, 53953, 54004, 54025, 54081, 54111, 54167, 54232, 54250, 54251, 54290, 54316, 54349, 54366, 54383, 54384, 54386, 54395, 54428, 54499, 54547, 54551, 54680, 54716, 54745, 54748, 54751, 54785, 54791, 54820, 54828, 54830, 54859, 54860, 54879, 54960, 55032, 55063, 55122, 55190, 55222, 55245, 55249, 55261, 55262, 55263, 55288, 55295, 55322, 55365, 55373, 55415, 55552, 55566, 55618, 55630, 55656, 55665, 55683, 55717, 55757, 55814, 55860, 55929, 55960, 55977, 55993, 56088, 56089, 56163, 56178, 56190, 56241, 56245, 56246, 56279, 56309, 56318, 56319, 56401, 56450, 56458, 56500, 56540, 56564, 56571, 56649, 56700, 56849, 56949, 56958, 56970, 56988, 57047, 57049, 57070, 57153, 57156, 57161, 57294, 57296, 57320, 57323, 57349, 57377, 57485, 57512, 57541, 57637, 57690, 57788, 57813, 57836, 57881, 57908, 57909, 57916, 57937, 57980, 58088, 58126, 58137, 58156, 58169, 58174, 58209, 58217, 58219, 58249, 58259, 58277, 58342, 58353, 58360, 58364, 58380, 58383, 58412, 58484, 58495, 58613, 58656, 58676, 58683, 58738, 58740, 58745, 58768, 58818, 58844, 58849, 58891, 58952, 58991, 59006, 59009, 59038, 59046, 59057, 59062, 59096, 59106, 59195, 59231, 59291, 59306, 59312, 59324, 59449, 59502, 59522, 59540, 59616, 59620, 59671, 59818, 59834, 59930, 59933, 59985, 60055, 60078, 60273, 60348, 60356, 60361, 60446, 60559, 60601, 60639, 60685, 60688, 60694, 60799, 60801, 60824, 60962, 61040, 61061, 61077, 61098, 61242, 61277, 61314, 61322, 61325, 61346, 61354, 61393, 61459, 61473, 61545, 61575, 61612, 61615, 61616, 61642, 61652, 61655, 61657, 61685, 61703, 61747, 61852, 61867, 61878, 61899, 61938, 61967, 61989, 62056, 62128, 62131, 62140, 62143, 62154, 62176, 62177, 62201, 62246, 62282, 62332, 62351, 62375, 62387, 62389, 62401, 62434, 62465, 62482, 62523, 62532, 62557, 62578, 62616, 62625, 62661, 62683, 62738, 62817, 62868, 62880, 62902, 62935, 62939, 63167, 63273, 63321, 63334, 63430, 63454, 63459, 63489, 63515, 63519, 63528, 63533, 63557, 63595, 63611, 63620, 63628, 63651, 63692, 63754, 63785, 63802, 63931, 63954, 64019, 64040, 64058, 64133, 64186, 64217, 64365, 64369, 64388, 64420, 64461, 64506, 64523, 64611, 64778, 64844, 64900, 64902, 64975, 64992, 65122, 65163, 65182, 65207, 65220, 65242, 65244, 65293, 65338, 65352, 65384, 65404, 65424, 65497, 65545, 65563, 65666, 65730, 65741, 65845, 65881, 65915, 65918, 65926, 65932, 65947, 66048, 66138, 66188, 66207, 66208, 66255, 66295, 66296, 66311, 66368, 66433, 66469, 66489, 66545, 66592, 66594, 66600, 66605, 66616, 66633, 66676, 66734, 66736, 66746, 66777, 66782, 66837, 66861, 66929, 67000, 67047, 67084, 67121, 67131, 67153, 67159, 67182, 67225, 67323, 67357, 67422, 67425, 67479, 67498, 67500, 67572, 67578, 67605, 67644, 67660, 67716, 67737, 67768, 67772, 67821, 67861, 67897, 67920, 67951, 67968, 67978, 67984, 68006, 68043, 68141, 68220, 68325, 68331, 68334, 68371, 68386, 68428, 68484, 68504, 68552, 68564, 68641, 68663, 68711, 68808, 68831, 68855, 68869, 69052, 69061, 69083, 69139, 69170, 69198, 69228, 69239, 69307, 69330, 69394, 69408, 69435, 69487, 69545, 69575, 69620, 69660, 69663, 69690, 69702, 69738, 69748, 69756, 69762, 69766, 69779, 69931, 69989, 70117, 70139, 70145, 70168, 70230, 70234, 70294, 70318, 70363, 70569, 70589, 70644, 70715, 70745, 70747, 70900, 71010, 71030, 71047, 71053, 71068, 71074, 71085, 71103, 71119, 71131, 71179, 71199, 71225, 71330, 71335, 71340, 71429, 71450, 71518, 71548, 71558, 71567, 71616, 71641, 71669, 71687, 71715, 71754, 71784, 71806, 71824, 71833, 71834, 71882, 71883, 71889, 71941, 71951, 71959, 71963, 71967, 71985, 71992, 72093, 72109, 72110, 72138, 72324, 72340, 72385, 72454, 72467, 72476, 72490, 72569, 72632, 72639, 72880, 72886, 72890, 72898, 72956, 73042, 73082, 73212, 73238, 73253, 73270, 73328, 73378, 73389, 73451, 73494, 73517, 73519, 73589, 73615, 73634, 73757, 73761, 73773, 73844, 73863, 73959, 74018, 74040, 74046, 74075, 74086, 74098, 74149, 74157, 74172, 74181, 74218, 74301, 74302, 74310, 74336, 74464, 74486, 74502, 74517, 74580, 74581, 74585, 74591, 74595, 74599, 74605, 74609, 74614, 74625, 74665, 74683, 74695, 74739, 74783, 74817, 74832, 74876, 74935, 74937, 74967, 74982, 74989, 74990, 75171, 75247, 75252, 75326, 75330, 75353, 75431, 75507, 75607, 75633, 75717, 75726, 75732, 75808, 75847, 75895, 75907, 75915, 75920, 75935, 75940, 75964, 75979, 76066, 76074, 76120, 76248, 76265, 76274, 76355, 76397, 76518, 76535, 76561, 76566, 76606, 76635, 76662, 76783, 76827, 76828, 76834, 76860, 76886, 76891, 77002, 77096, 77170, 77194, 77199, 77250, 77268, 77345, 77515, 77553, 77634, 77644, 77657, 77692, 77753, 77755, 77758, 77759, 77796, 77814, 77817, 77825, 77857, 77864, 77965, 78010, 78072, 78078, 78142, 78184, 78194, 78219, 78356, 78372, 78393, 78399, 78439, 78447, 78450, 78474, 78494, 78585, 78669, 78728, 78796, 78833, 78929, 78932, 78964, 78998, 79018, 79143, 79165, 79183, 79192, 79321, 79326, 79335, 79349, 79410, 79437, 79441, 79452, 79468, 79520, 79602, 79622, 79639, 79646, 79715, 79727, 79817, 79849, 79871, 79916, 79920] not in index'"
     ]
    }
   ],
   "source": [
    "X_trains = []\n",
    "y_trains = []\n",
    "for train,test in splits:\n",
    "    b = test\n",
    "    print(len(b))\n",
    "    mask = np.ones(len(X_train), dtype=bool)\n",
    "    mask[b,] = False\n",
    "    x, y = X_train[b], y_train[b]# instead of a[b] you could also do a[~mask]\n",
    "    #print(len(x))\n",
    "    X_trains.append(x)\n",
    "    y_trains.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4442e8a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_trains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "abe9715a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNN:\n",
    "    model_3 = Sequential()#MLPClassifier(hidden_layer_sizes=(115, 58, 29), max_iter=1000, alpha=0.0001, solver='adam', verbose=10, random_state=21, tol=0.000000001)\n",
    "    def __init__(self,first=False,name=None):\n",
    "        if first == True:\n",
    "            # Model_3 with Batch Normalization\n",
    "#             self.model_3 = MLPClassifier(hidden_layer_sizes=(115, 58, 29), max_iter=1000, alpha=0.0001, solver='adam', verbose=10, random_state=21, tol=0.000000001)\n",
    "            self.model_3 = Sequential()\n",
    "            self.model_3.add(Dense(115, input_shape=(115,), activation='elu'))\n",
    "            self.model_3.add(Dense(58, input_shape=(115,), activation='elu'))\n",
    "            self.model_3.add(Dense(29, input_shape=(115,), activation='elu'))\n",
    "            self.model_3.add(Dense(1, input_shape=(115,), activation='sigmoid'))\n",
    "            self.model_3.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            self.model_3 = keras.models.load_model('Himanshu\\models\\model_{}'.format(name))\n",
    "            \n",
    "    def fit(self, X_train, y_train, epoch, first=False, name=None):\n",
    "#         print(\"X_train shape = {}\".format(X_train.shape))\n",
    "#         print(\"X_test shape = {}\".format(X_test.shape))\n",
    "#         print(\"Y_train shape = {}\".format(Y_train.shape))\n",
    "#         print(\"Y_test shape = {}\".format(Y_test.shape))\n",
    "#         tf.config.run_functions_eagerly(True)\n",
    "        print(\"==========================================================================\")\n",
    "        print(\"Training model {}\".format(name))\n",
    "#         callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "#         filepath='Himanshu\\models\\model_{}\\checkpoint'.format(name),\n",
    "#         save_weights_only=True,\n",
    "#         monitor='val_accuracy',\n",
    "#         mode='max',\n",
    "#         save_best_only=True)\n",
    "        if first == False:\n",
    "            self.model_3 = keras.models.load_model('Himanshu\\models\\model_{}'.format(name))\n",
    "        self.model_3.fit(X_train, y_train,epochs=epoch)\n",
    "#         self.model_3.load_weights('Himanshu\\models\\model_{}\\checkpoint'.format(name))\n",
    "    \n",
    "    def send_update(self):\n",
    "        return self.model_3\n",
    "    \n",
    "    def get_update(self,weights,name):\n",
    "        self.model_3.set_weights(weights)\n",
    "        self.model_3.save('Himanshu\\models\\model_{}'.format(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "b055bbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate(model_list):\n",
    "    weights = [model.get_weights() for model in model_list]\n",
    "    #print(weights)\n",
    "    new_weights = list()\n",
    "    for weights_list_tuple in zip(*weights): \n",
    "        new_weights.append(\n",
    "            np.array([np.array(w).mean(axis=0) for w in zip(*weights_list_tuple)])\n",
    "        )\n",
    "    return new_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "d267b73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLIENTS = 8\n",
    "K = 8\n",
    "NUM_ROUNDS = 10\n",
    "NUM_EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b158613f",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "for client in range(NUM_CLIENTS):\n",
    "    modelx = MyNN(True, name=str(client))\n",
    "    models.append(modelx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "6f9c6627",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_model = MyNN(True, 'global')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "6ad9b510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============TRAINING STARTED AT: 2023-02-23 21:38:17.913422 ============\n",
      "=====================ROUND NO 0 ====================================\n",
      "\n",
      "==========================================================================\n",
      "Training model 0\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 41s 2ms/step - loss: 0.0029 - accuracy: 0.9994\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 43s 2ms/step - loss: 0.0016 - accuracy: 0.9996A: 37s - loss:  - ETA: 37s -  - ETA: 13s - loss: 0.0017 - accura - ETA: 13s - loss: 0.0018 - accuracy: 0. - ETA: 12s - loss: 0.0017 - ETA: 11s - loss - ETA: 5s - loss: 0.0017 - accuracy: 0.99 - ETA: 5s - - ETA: 4s - loss: 0.0017 \n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 37s 2ms/step - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 36s 1ms/step - loss: 9.3715e-04 - accuracy: 0.9997\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 37s 1ms/step - loss: 8.3560e-04 - accuracy: 0.9997\n",
      "==========================================================================\n",
      "Training model 1\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 39s 2ms/step - loss: 0.0030 - accuracy: 0.9993\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 39s 2ms/step - loss: 0.0014 - accuracy: 0.9996\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 38s 2ms/step - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 40s 2ms/step - loss: 0.0010 - accuracy: 0.9997 1s - loss: 0.0\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 42s 2ms/step - loss: 9.4024e-04 - accuracy: 0.9997\n",
      "==========================================================================\n",
      "Training model 2\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 44s 2ms/step - loss: 0.0030 - accuracy: 0.9994A: 29s  - ETA: 27s - loss: 0.0042 - accuracy: \n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 43s 2ms/step - loss: 0.0017 - accuracy: 0.9996 2s -\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 43s 2ms/step - loss: 0.0012 - accuracy: 0.9997E - ETA - ETA: 3s - loss: 0.0012 - accuracy - ETA: 3s - loss: 0.0012 - accu - ETA: 3s - loss: 0.0012 - accura - ETA: 2s - los - ETA: 2s -\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 41s 2ms/step - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 0.0010 - accuracy: 0.9997 1s - loss: 0.0010 - accuracy: 0. - ETA: 0s - loss: 0.0\n",
      "==========================================================================\n",
      "Training model 3\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 35s 1ms/step - loss: 0.0030 - accuracy: 0.9994\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 37s 1ms/step - loss: 0.0015 - accuracy: 0.9996\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 38s 2ms/step - loss: 0.0013 - accuracy: 0.9996\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 38s 2ms/step - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 39s 2ms/step - loss: 0.0011 - accuracy: 0.9996\n",
      "==========================================================================\n",
      "Training model 4\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 40s 2ms/step - loss: 0.0030 - accuracy: 0.9993\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 41s 2ms/step - loss: 0.0017 - accuracy: 0.9996\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 41s 2ms/step - loss: 0.0012 - accuracy: 0.9996 0s - loss: 0.0012 - accuracy:  - ETA: 0s - los\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 41s 2ms/step - loss: 0.0010 - accuracy: 0.9997\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 42s 2ms/step - loss: 9.0840e-04 - accuracy: 0.9997A: 36s  - ETA: 31s - los - ETA: 30s - loss: 0.0010 -  - ETA: 29s - loss: 0.0011 - accuracy:   - ETA: 23s - loss: 0.0010 - accu - ETA: 6s - - ETA: 2s - loss: 9.0606e\n",
      "==========================================================================\n",
      "Training model 5\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 42s 2ms/step - loss: 0.0030 - accuracy: 0.9994A: 30s - lo\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 41s 2ms/step - loss: 0.0018 - accuracy: 0.9996\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 41s 2ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 42s 2ms/step - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 42s 2ms/step - loss: 9.5530e-04 - accuracy: 0.9997\n",
      "==========================================================================\n",
      "Training model 6\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 34s 1ms/step - loss: 0.0032 - accuracy: 0.9993\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 28s 1ms/step - loss: 0.0018 - accuracy: 0.9996\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 28s 1ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 28s 1ms/step - loss: 0.0010 - accuracy: 0.9997\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 28s 1ms/step - loss: 9.0676e-04 - accuracy: 0.9997\n",
      "==========================================================================\n",
      "Training model 7\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 29s 1ms/step - loss: 0.0031 - accuracy: 0.9993\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 28s 1ms/step - loss: 0.0015 - accuracy: 0.9996\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 28s 1ms/step - loss: 0.0011 - accuracy: 0.9996\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 28s 1ms/step - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 28s 1ms/step - loss: 9.4471e-04 - accuracy: 0.9997\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_0\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_1\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_2\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_3\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_4\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_5\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_6\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_7\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\global_model\\assets\n",
      "=====================ROUND NO 1 ====================================\n",
      "\n",
      "==========================================================================\n",
      "Training model 0\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 42s 2ms/step - loss: 0.0022 - accuracy: 0.9993\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 39s 2ms/step - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 39s 2ms/step - loss: 9.2579e-04 - accuracy: 0.9997\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 39s 2ms/step - loss: 8.9236e-04 - accuracy: 0.9997\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 39s 2ms/step - loss: 8.0866e-04 - accuracy: 0.9998\n",
      "==========================================================================\n",
      "Training model 1\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 40s 2ms/step - loss: 0.0021 - accuracy: 0.9993\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 39s 2ms/step - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 39s 2ms/step - loss: 0.0010 - accuracy: 0.9997\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 39s 2ms/step - loss: 9.3883e-04 - accuracy: 0.9997\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 39s 2ms/step - loss: 8.4730e-04 - accuracy: 0.9997\n",
      "==========================================================================\n",
      "Training model 2\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 40s 2ms/step - loss: 0.0022 - accuracy: 0.9993\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 39s 2ms/step - loss: 0.0012 - accuracy: 0.9997\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 39s 2ms/step - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 31s 1ms/step - loss: 0.0010 - accuracy: 0.9997\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 31s 1ms/step - loss: 9.7463e-04 - accuracy: 0.9997\n",
      "==========================================================================\n",
      "Training model 3\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 31s 1ms/step - loss: 0.0022 - accuracy: 0.9993\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 30s 1ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 30s 1ms/step - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 42s 2ms/step - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 37s 1ms/step - loss: 9.8144e-04 - accuracy: 0.9997\n",
      "==========================================================================\n",
      "Training model 4\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 38s 2ms/step - loss: 0.0022 - accuracy: 0.9993\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 37s 1ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 37s 2ms/step - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 37s 2ms/step - loss: 9.5790e-04 - accuracy: 0.9997\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 37s 1ms/step - loss: 8.9717e-04 - accuracy: 0.9997\n",
      "==========================================================================\n",
      "Training model 5\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 38s 2ms/step - loss: 0.0022 - accuracy: 0.9993\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 37s 2ms/step - loss: 0.0013 - accuracy: 0.9996\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 37s 2ms/step - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 37s 2ms/step - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 37s 2ms/step - loss: 9.6503e-04 - accuracy: 0.9997\n",
      "==========================================================================\n",
      "Training model 6\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 37s 1ms/step - loss: 0.0022 - accuracy: 0.9993\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 37s 2ms/step - loss: 0.0011 - accuracy: 0.9996\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 37s 2ms/step - loss: 0.0011 - accuracy: 0.9996\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 37s 2ms/step - loss: 9.8323e-04 - accuracy: 0.9996\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 34s 1ms/step - loss: 9.3481e-04 - accuracy: 0.9997\n",
      "==========================================================================\n",
      "Training model 7\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 31s 1ms/step - loss: 0.0022 - accuracy: 0.9993\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 31s 1ms/step - loss: 0.0011 - accuracy: 0.9996\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 30s 1ms/step - loss: 0.0011 - accuracy: 0.9996\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 31s 1ms/step - loss: 9.9554e-04 - accuracy: 0.9997\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 31s 1ms/step - loss: 8.9015e-04 - accuracy: 0.9997\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_0\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_1\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_2\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_3\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_4\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_5\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_6\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_7\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\global_model\\assets\n",
      "=====================ROUND NO 2 ====================================\n",
      "\n",
      "==========================================================================\n",
      "Training model 0\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 31s 1ms/step - loss: 9.1126e-04 - accuracy: 0.9997\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 36s 1ms/step - loss: 7.8856e-04 - accuracy: 0.9998\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 40s 2ms/step - loss: 7.4175e-04 - accuracy: 0.9998\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 38s 2ms/step - loss: 6.9990e-04 - accuracy: 0.9998\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 6.0229e-04 - accuracy: 0.9998\n",
      "==========================================================================\n",
      "Training model 1\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 31s 1ms/step - loss: 9.0315e-04 - accuracy: 0.9997\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 31s 1ms/step - loss: 7.8924e-04 - accuracy: 0.9998\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 30s 1ms/step - loss: 7.4979e-04 - accuracy: 0.9998\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 30s 1ms/step - loss: 6.8326e-04 - accuracy: 0.9998\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 30s 1ms/step - loss: 7.1742e-04 - accuracy: 0.9998\n",
      "==========================================================================\n",
      "Training model 2\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 0.0010 - accuracy: 0.9997\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 31s 1ms/step - loss: 8.7756e-04 - accuracy: 0.9997\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 31s 1ms/step - loss: 8.9018e-04 - accuracy: 0.9997\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 31s 1ms/step - loss: 8.0450e-04 - accuracy: 0.9998\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 31s 1ms/step - loss: 7.7312e-04 - accuracy: 0.9998\n",
      "==========================================================================\n",
      "Training model 3\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 31s 1ms/step - loss: 0.0010 - accuracy: 0.9997\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 31s 1ms/step - loss: 8.9619e-04 - accuracy: 0.9997\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 31s 1ms/step - loss: 8.0552e-04 - accuracy: 0.9998\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 31s 1ms/step - loss: 8.0367e-04 - accuracy: 0.9998\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 31s 1ms/step - loss: 7.8258e-04 - accuracy: 0.9998\n",
      "==========================================================================\n",
      "Training model 4\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 0.0010 - accuracy: 0.9997\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 31s 1ms/step - loss: 9.1027e-04 - accuracy: 0.9997\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 31s 1ms/step - loss: 7.8764e-04 - accuracy: 0.9998\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 31s 1ms/step - loss: 8.0091e-04 - accuracy: 0.9997\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 31s 1ms/step - loss: 7.4721e-04 - accuracy: 0.9998\n",
      "==========================================================================\n",
      "Training model 5\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 0.0010 - accuracy: 0.9997\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 8.8714e-04 - accuracy: 0.9997\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 8.0383e-04 - accuracy: 0.9998\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 7.4021e-04 - accuracy: 0.9998\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 31s 1ms/step - loss: 6.7873e-04 - accuracy: 0.9998\n",
      "==========================================================================\n",
      "Training model 6\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 0.0010 - accuracy: 0.9997\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 8.8750e-04 - accuracy: 0.9997\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 7.7324e-04 - accuracy: 0.9998\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24669/24669 [==============================] - 32s 1ms/step - loss: 7.4915e-04 - accuracy: 0.9998\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 6.9689e-04 - accuracy: 0.9998\n",
      "==========================================================================\n",
      "Training model 7\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 9.4072e-04 - accuracy: 0.9997\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 8.3649e-04 - accuracy: 0.9997\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 7.7274e-04 - accuracy: 0.9998\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 31s 1ms/step - loss: 7.0620e-04 - accuracy: 0.9998\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 6.2706e-04 - accuracy: 0.9998\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_0\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_1\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_2\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_3\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_4\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_5\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_6\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_7\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\global_model\\assets\n",
      "=====================ROUND NO 3 ====================================\n",
      "\n",
      "==========================================================================\n",
      "Training model 0\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 6.7811e-04 - accuracy: 0.9998\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 6.1378e-04 - accuracy: 0.9998\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 6.6704e-04 - accuracy: 0.9998\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.1719e-04 - accuracy: 0.9999\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 4.7632e-04 - accuracy: 0.9999\n",
      "==========================================================================\n",
      "Training model 1\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 7.0112e-04 - accuracy: 0.9998\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 31s 1ms/step - loss: 6.4764e-04 - accuracy: 0.9998\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 6.0483e-04 - accuracy: 0.9998\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 31s 1ms/step - loss: 5.5117e-04 - accuracy: 0.9998\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 31s 1ms/step - loss: 6.3700e-04 - accuracy: 0.9998\n",
      "==========================================================================\n",
      "Training model 2\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 7.3418e-04 - accuracy: 0.9998\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 31s 1ms/step - loss: 7.0274e-04 - accuracy: 0.9998\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 6.7282e-04 - accuracy: 0.9998\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 6.5846e-04 - accuracy: 0.9998\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 31s 1ms/step - loss: 5.8941e-04 - accuracy: 0.9998\n",
      "==========================================================================\n",
      "Training model 3\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 7.9326e-04 - accuracy: 0.9998\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 31s 1ms/step - loss: 6.9274e-04 - accuracy: 0.9998\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 31s 1ms/step - loss: 7.9938e-04 - accuracy: 0.9998\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 31s 1ms/step - loss: 6.5473e-04 - accuracy: 0.9998\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 31s 1ms/step - loss: 6.2578e-04 - accuracy: 0.9998\n",
      "==========================================================================\n",
      "Training model 4\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 7.8499e-04 - accuracy: 0.9998\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 7.2823e-04 - accuracy: 0.9998\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 31s 1ms/step - loss: 6.5366e-04 - accuracy: 0.9998\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 31s 1ms/step - loss: 5.7486e-04 - accuracy: 0.9999\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 31s 1ms/step - loss: 6.0591e-04 - accuracy: 0.9998\n",
      "==========================================================================\n",
      "Training model 5\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 7.1029e-04 - accuracy: 0.9998\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 6.6550e-04 - accuracy: 0.9998\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 6.3348e-04 - accuracy: 0.9998\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.6053e-04 - accuracy: 0.9998\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.8103e-04 - accuracy: 0.9998\n",
      "==========================================================================\n",
      "Training model 6\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 7.3803e-04 - accuracy: 0.9998\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 6.7568e-04 - accuracy: 0.9998\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.9193e-04 - accuracy: 0.9998\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.8730e-04 - accuracy: 0.9998\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.6377e-04 - accuracy: 0.9998\n",
      "==========================================================================\n",
      "Training model 7\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 6.9715e-04 - accuracy: 0.9998\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.7985e-04 - accuracy: 0.9998\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 6.6547e-04 - accuracy: 0.9998\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.6038e-04 - accuracy: 0.9998\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.8510e-04 - accuracy: 0.9999\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_0\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_1\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_2\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_3\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_4\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_5\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_6\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_7\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\global_model\\assets\n",
      "=====================ROUND NO 4 ====================================\n",
      "\n",
      "==========================================================================\n",
      "Training model 0\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 5.4293e-04 - accuracy: 0.9999\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 4.8814e-04 - accuracy: 0.9999\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 4.4164e-04 - accuracy: 0.9999\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 4.5959e-04 - accuracy: 0.9999\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.2110e-04 - accuracy: 0.9999\n",
      "==========================================================================\n",
      "Training model 1\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.9991e-04 - accuracy: 0.9999\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.6461e-04 - accuracy: 0.9999\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.7653e-04 - accuracy: 0.9999\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.5509e-04 - accuracy: 0.9999\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 6.2332e-04 - accuracy: 0.9999\n",
      "==========================================================================\n",
      "Training model 2\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 6.9069e-04 - accuracy: 0.9998\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.8814e-04 - accuracy: 0.9998\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.9712e-04 - accuracy: 0.9998\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.6121e-04 - accuracy: 0.9998\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.7709e-04 - accuracy: 0.9998\n",
      "==========================================================================\n",
      "Training model 3\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 6.8955e-04 - accuracy: 0.9998\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.9952e-04 - accuracy: 0.9998\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 6.0233e-04 - accuracy: 0.9998\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.5595e-04 - accuracy: 0.9998\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.3122e-04 - accuracy: 0.9998\n",
      "==========================================================================\n",
      "Training model 4\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 6.2247e-04 - accuracy: 0.9998\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 31s 1ms/step - loss: 6.1136e-04 - accuracy: 0.9998\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.8068e-04 - accuracy: 0.9999\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.4499e-04 - accuracy: 0.9999\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 4.9262e-04 - accuracy: 0.9999\n",
      "==========================================================================\n",
      "Training model 5\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.9018e-04 - accuracy: 0.9998\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 6.1216e-04 - accuracy: 0.9998\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.3437e-04 - accuracy: 0.9998\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 6.1295e-04 - accuracy: 0.9998\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 4.9171e-04 - accuracy: 0.9999\n",
      "==========================================================================\n",
      "Training model 6\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 6.1685e-04 - accuracy: 0.9998\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.4184e-04 - accuracy: 0.9998\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 6.0275e-04 - accuracy: 0.9999\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.5866e-04 - accuracy: 0.9999\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.5251e-04 - accuracy: 0.9999\n",
      "==========================================================================\n",
      "Training model 7\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 6.4854e-04 - accuracy: 0.9998\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.0503e-04 - accuracy: 0.9999\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.3710e-04 - accuracy: 0.9999\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.4820e-04 - accuracy: 0.9999\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.1383e-04 - accuracy: 0.9999\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_0\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_1\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_2\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_3\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_4\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_5\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_6\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_7\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\global_model\\assets\n",
      "=====================ROUND NO 5 ====================================\n",
      "\n",
      "==========================================================================\n",
      "Training model 0\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 5.1967e-04 - accuracy: 0.9999\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 4.4874e-04 - accuracy: 0.9999\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 4.6756e-04 - accuracy: 0.9999\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 4.4980e-04 - accuracy: 0.9999\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 4.4769e-04 - accuracy: 0.9999\n",
      "==========================================================================\n",
      "Training model 1\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.7508e-04 - accuracy: 0.9999\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.1872e-04 - accuracy: 0.9999\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.1756e-04 - accuracy: 0.9999\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.4467e-04 - accuracy: 0.9999\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.1232e-04 - accuracy: 0.9999\n",
      "==========================================================================\n",
      "Training model 2\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.8907e-04 - accuracy: 0.9998\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.6053e-04 - accuracy: 0.9999\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.5474e-04 - accuracy: 0.9999\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.7503e-04 - accuracy: 0.9999\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.5327e-04 - accuracy: 0.9998\n",
      "==========================================================================\n",
      "Training model 3\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.5543e-04 - accuracy: 0.9998\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 31s 1ms/step - loss: 5.4193e-04 - accuracy: 0.9998\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.8534e-04 - accuracy: 0.9999\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.2998e-04 - accuracy: 0.9999\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.8479e-04 - accuracy: 0.9999\n",
      "==========================================================================\n",
      "Training model 4\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.6169e-04 - accuracy: 0.9999\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 4.8819e-04 - accuracy: 0.9999\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.1714e-04 - accuracy: 0.9999\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 4.7602e-04 - accuracy: 0.9999\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.7017e-04 - accuracy: 0.9999\n",
      "==========================================================================\n",
      "Training model 5\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.4075e-04 - accuracy: 0.9999\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24669/24669 [==============================] - 32s 1ms/step - loss: 4.7332e-04 - accuracy: 0.9999\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.4120e-04 - accuracy: 0.9999\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 4.6423e-04 - accuracy: 0.9999\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.1935e-04 - accuracy: 0.9999\n",
      "==========================================================================\n",
      "Training model 6\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 34s 1ms/step - loss: 5.3584e-04 - accuracy: 0.9999\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 5.1787e-04 - accuracy: 0.9999\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 6.1047e-04 - accuracy: 0.9998\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 4.9194e-04 - accuracy: 0.9999\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.1825e-04 - accuracy: 0.9999\n",
      "==========================================================================\n",
      "Training model 7\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 5.5063e-04 - accuracy: 0.9999\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 5.4697e-04 - accuracy: 0.9998\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 5.3556e-04 - accuracy: 0.9999\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 5.1854e-04 - accuracy: 0.9999\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 5.7471e-04 - accuracy: 0.9999\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_0\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_1\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_2\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_3\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_4\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_5\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_6\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_7\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\global_model\\assets\n",
      "=====================ROUND NO 6 ====================================\n",
      "\n",
      "==========================================================================\n",
      "Training model 0\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 4.7416e-04 - accuracy: 0.9999\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 4.3246e-04 - accuracy: 0.9999\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 4.8376e-04 - accuracy: 0.9999\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 4.6425e-04 - accuracy: 0.9999\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 4.5192e-04 - accuracy: 0.9999\n",
      "==========================================================================\n",
      "Training model 1\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 4.8437e-04 - accuracy: 0.9999\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 4.9166e-04 - accuracy: 0.9999\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 4.6118e-04 - accuracy: 0.9999\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 4.4090e-04 - accuracy: 0.9999\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.5087e-04 - accuracy: 0.9999\n",
      "==========================================================================\n",
      "Training model 2\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 5.8298e-04 - accuracy: 0.9999\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 5.5103e-04 - accuracy: 0.9999\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.9190e-04 - accuracy: 0.9999\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.6340e-04 - accuracy: 0.9999\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.5597e-04 - accuracy: 0.9999\n",
      "==========================================================================\n",
      "Training model 3\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.8175e-04 - accuracy: 0.9999\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 6.9034e-04 - accuracy: 0.9999\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.0803e-04 - accuracy: 0.9999\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.3178e-04 - accuracy: 0.9999\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.6418e-04 - accuracy: 0.9999\n",
      "==========================================================================\n",
      "Training model 4\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.2241e-04 - accuracy: 0.9999\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.5413e-04 - accuracy: 0.9999\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 4.9917e-04 - accuracy: 0.9999\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.8614e-04 - accuracy: 0.9999\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.1149e-04 - accuracy: 0.9999\n",
      "==========================================================================\n",
      "Training model 5\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.2186e-04 - accuracy: 0.9999\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.1039e-04 - accuracy: 0.9999\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.1265e-04 - accuracy: 0.9999\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 4.8813e-04 - accuracy: 0.9999\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.2974e-04 - accuracy: 0.9999\n",
      "==========================================================================\n",
      "Training model 6\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.1564e-04 - accuracy: 0.9999\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.2191e-04 - accuracy: 0.9999\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.0549e-04 - accuracy: 0.9999\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 4.6394e-04 - accuracy: 0.9999\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 4.4376e-04 - accuracy: 0.9999\n",
      "==========================================================================\n",
      "Training model 7\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 4.9068e-04 - accuracy: 0.9999\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 5.9024e-04 - accuracy: 0.9999\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 4.3032e-04 - accuracy: 0.9999\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 5.8783e-04 - accuracy: 0.9999\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 4.9784e-04 - accuracy: 0.9999\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_0\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_1\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_2\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_3\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_4\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_5\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_6\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_7\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\global_model\\assets\n",
      "=====================ROUND NO 7 ====================================\n",
      "\n",
      "==========================================================================\n",
      "Training model 0\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 5.7293e-04 - accuracy: 0.9999\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.3481e-04 - accuracy: 0.9999\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 3.8763e-04 - accuracy: 0.9999\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 5.4632e-04 - accuracy: 0.9999\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 4.6416e-04 - accuracy: 0.9999\n",
      "==========================================================================\n",
      "Training model 1\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 5.3876e-04 - accuracy: 0.9999\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 5.5345e-04 - accuracy: 0.9999\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 4.9737e-04 - accuracy: 0.9999\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 5.6713e-04 - accuracy: 0.9999\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 5.5750e-04 - accuracy: 0.9999\n",
      "==========================================================================\n",
      "Training model 2\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 5.8162e-04 - accuracy: 0.9999\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 6.1273e-04 - accuracy: 0.9999\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 4.8641e-04 - accuracy: 0.9999\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 4.9037e-04 - accuracy: 0.9999\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 4.9581e-04 - accuracy: 0.9999\n",
      "==========================================================================\n",
      "Training model 3\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 6.0563e-04 - accuracy: 0.9999\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 4.9501e-04 - accuracy: 0.9999\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 4.9166e-04 - accuracy: 0.9999\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 5.1890e-04 - accuracy: 0.9999\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 7.5532e-04 - accuracy: 0.9999\n",
      "==========================================================================\n",
      "Training model 4\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 5.2020e-04 - accuracy: 0.9999\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.3231e-04 - accuracy: 0.9999\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 5.0798e-04 - accuracy: 0.9999\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 3.9847e-04 - accuracy: 0.9999\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 5.1025e-04 - accuracy: 0.9999\n",
      "==========================================================================\n",
      "Training model 5\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.0992e-04 - accuracy: 0.9999\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 4.9094e-04 - accuracy: 0.9999\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.6244e-04 - accuracy: 0.9999\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.0489e-04 - accuracy: 0.9999\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 4.8960e-04 - accuracy: 0.9999\n",
      "==========================================================================\n",
      "Training model 6\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 4.9563e-04 - accuracy: 0.9999\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 4.8897e-04 - accuracy: 0.9999\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 4.9078e-04 - accuracy: 0.9999\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 4.5049e-04 - accuracy: 0.9999\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 5.0288e-04 - accuracy: 0.9999\n",
      "==========================================================================\n",
      "Training model 7\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 5.2398e-04 - accuracy: 0.9999\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.3196e-04 - accuracy: 0.9999\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.1269e-04 - accuracy: 0.9999\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.5005e-04 - accuracy: 0.9999\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 4.5791e-04 - accuracy: 0.9999\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_0\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_1\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_2\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_3\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_4\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_5\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_6\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_7\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\global_model\\assets\n",
      "=====================ROUND NO 8 ====================================\n",
      "\n",
      "==========================================================================\n",
      "Training model 0\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 5.0944e-04 - accuracy: 0.9999\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 3.8503e-04 - accuracy: 0.9999\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 4.6292e-04 - accuracy: 0.9999\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 5.1457e-04 - accuracy: 0.9999\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 4.9264e-04 - accuracy: 0.9999\n",
      "==========================================================================\n",
      "Training model 1\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 5.4726e-04 - accuracy: 0.9999\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 7.3571e-04 - accuracy: 0.9999\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 4.8079e-04 - accuracy: 0.9999\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 5.7160e-04 - accuracy: 0.9999\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.3426e-04 - accuracy: 0.9999\n",
      "==========================================================================\n",
      "Training model 2\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 6.0312e-04 - accuracy: 0.9999\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.5911e-04 - accuracy: 0.9999\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 5.4037e-04 - accuracy: 0.9999\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 5.0058e-04 - accuracy: 0.9999\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 5.5349e-04 - accuracy: 0.9999\n",
      "==========================================================================\n",
      "Training model 3\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 34s 1ms/step - loss: 6.8763e-04 - accuracy: 0.9999\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 6.1310e-04 - accuracy: 0.9999\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 6.1471e-04 - accuracy: 0.9999\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 5.3082e-04 - accuracy: 0.9999\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 5.8730e-04 - accuracy: 0.9999\n",
      "==========================================================================\n",
      "Training model 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 5.8846e-04 - accuracy: 0.9999\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 5.1012e-04 - accuracy: 0.9999\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 5.2087e-04 - accuracy: 0.9999\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 5.5283e-04 - accuracy: 0.9999\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 5.4042e-04 - accuracy: 0.9999\n",
      "==========================================================================\n",
      "Training model 5\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 5.4703e-04 - accuracy: 0.9999\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 4.6180e-04 - accuracy: 0.9999\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 5.2171e-04 - accuracy: 0.9999\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.2341e-04 - accuracy: 0.9999\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 5.6556e-04 - accuracy: 0.9999\n",
      "==========================================================================\n",
      "Training model 6\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 5.1463e-04 - accuracy: 0.9999\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 4.9125e-04 - accuracy: 0.9999\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.5517e-04 - accuracy: 0.9999\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 4.9598e-04 - accuracy: 0.9999\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.0808e-04 - accuracy: 0.9999\n",
      "==========================================================================\n",
      "Training model 7\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 6.3849e-04 - accuracy: 0.9999\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 32s 1ms/step - loss: 5.0597e-04 - accuracy: 0.9999\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 4.8882e-04 - accuracy: 0.9999\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 4.4929e-04 - accuracy: 0.9999\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 33s 1ms/step - loss: 5.4859e-04 - accuracy: 0.9999\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_0\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_1\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_2\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_3\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_4\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_5\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_6\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_7\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\global_model\\assets\n",
      "=====================ROUND NO 9 ====================================\n",
      "\n",
      "==========================================================================\n",
      "Training model 0\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 35s 1ms/step - loss: 5.8873e-04 - accuracy: 0.9998\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 35s 1ms/step - loss: 5.8401e-04 - accuracy: 0.9998\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 34s 1ms/step - loss: 4.5361e-04 - accuracy: 0.9999\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 34s 1ms/step - loss: 5.9946e-04 - accuracy: 0.9999\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 35s 1ms/step - loss: 4.1337e-04 - accuracy: 0.9999\n",
      "==========================================================================\n",
      "Training model 1\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 35s 1ms/step - loss: 6.0731e-04 - accuracy: 0.9999\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 35s 1ms/step - loss: 5.6872e-04 - accuracy: 0.9999\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 35s 1ms/step - loss: 7.6988e-04 - accuracy: 0.9999\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 35s 1ms/step - loss: 5.2074e-04 - accuracy: 0.9999\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 35s 1ms/step - loss: 6.1573e-04 - accuracy: 0.9998\n",
      "==========================================================================\n",
      "Training model 2\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 35s 1ms/step - loss: 6.3539e-04 - accuracy: 0.9999\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 35s 1ms/step - loss: 5.0505e-04 - accuracy: 0.9999\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 35s 1ms/step - loss: 6.1145e-04 - accuracy: 0.9999\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 35s 1ms/step - loss: 7.0802e-04 - accuracy: 0.9999\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 35s 1ms/step - loss: 5.5557e-04 - accuracy: 0.9999\n",
      "==========================================================================\n",
      "Training model 3\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 34s 1ms/step - loss: 6.6175e-04 - accuracy: 0.9999\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 34s 1ms/step - loss: 6.5442e-04 - accuracy: 0.9998\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 35s 1ms/step - loss: 6.0415e-04 - accuracy: 0.9999\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 35s 1ms/step - loss: 5.9541e-04 - accuracy: 0.9999\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 35s 1ms/step - loss: 6.9303e-04 - accuracy: 0.9998\n",
      "==========================================================================\n",
      "Training model 4\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 35s 1ms/step - loss: 6.0090e-04 - accuracy: 0.9999\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 34s 1ms/step - loss: 6.4922e-04 - accuracy: 0.9999\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 34s 1ms/step - loss: 4.9177e-04 - accuracy: 0.9999\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 34s 1ms/step - loss: 5.6210e-04 - accuracy: 0.9999\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 34s 1ms/step - loss: 5.6128e-04 - accuracy: 0.9999\n",
      "==========================================================================\n",
      "Training model 5\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 34s 1ms/step - loss: 5.1718e-04 - accuracy: 0.9999\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 34s 1ms/step - loss: 9.7180e-04 - accuracy: 0.9999\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 34s 1ms/step - loss: 5.0011e-04 - accuracy: 0.9999\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 34s 1ms/step - loss: 5.2238e-04 - accuracy: 0.9999\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 34s 1ms/step - loss: 4.9216e-04 - accuracy: 0.9999\n",
      "==========================================================================\n",
      "Training model 6\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 34s 1ms/step - loss: 6.4822e-04 - accuracy: 0.9999\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 34s 1ms/step - loss: 5.6319e-04 - accuracy: 0.9999\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 34s 1ms/step - loss: 6.0432e-04 - accuracy: 0.9999\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 34s 1ms/step - loss: 4.3578e-04 - accuracy: 0.9999\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 34s 1ms/step - loss: 4.8140e-04 - accuracy: 0.9999\n",
      "==========================================================================\n",
      "Training model 7\n",
      "Epoch 1/5\n",
      "24669/24669 [==============================] - 35s 1ms/step - loss: 7.3367e-04 - accuracy: 0.9999\n",
      "Epoch 2/5\n",
      "24669/24669 [==============================] - 35s 1ms/step - loss: 6.8629e-04 - accuracy: 0.9999\n",
      "Epoch 3/5\n",
      "24669/24669 [==============================] - 35s 1ms/step - loss: 5.9993e-04 - accuracy: 0.9999\n",
      "Epoch 4/5\n",
      "24669/24669 [==============================] - 34s 1ms/step - loss: 5.8717e-04 - accuracy: 0.9999\n",
      "Epoch 5/5\n",
      "24669/24669 [==============================] - 35s 1ms/step - loss: 5.2448e-04 - accuracy: 0.9999\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_0\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_1\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_2\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_3\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_4\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_5\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_6\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\model_7\\assets\n",
      "INFO:tensorflow:Assets written to: Himanshu\\models\\global_model\\assets\n",
      "********TRAINING COMPLETED*********** AT 2023-02-24 01:22:46.689330\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import random\n",
    "print(\"============TRAINING STARTED AT: {} ============\".format(datetime.datetime.now()))\n",
    "for rounds in range(NUM_ROUNDS):\n",
    "    print(\"=====================ROUND NO \"+str(rounds)+\" ====================================\\n\")\n",
    "    #K_arr = random.sample(range(0, NUM_CLIENTS), K)\n",
    "    model_list = []\n",
    "    for client in range(NUM_CLIENTS):\n",
    "        models[client].fit(X_trains[client],y_trains[client],NUM_EPOCHS, first = (rounds==0),name=str(client))\n",
    "        #first_array[client] = False\n",
    "        model_list.append(models[client].send_update())\n",
    "    new_weights = aggregate(model_list)\n",
    "#     global_model = clone_model(model_list[0])\n",
    "    global_model.model_3.set_weights(new_weights)\n",
    "    for client in range(NUM_CLIENTS):\n",
    "        models[client].get_update(new_weights,str(client))\n",
    "    global_model.model_3.save('Himanshu\\models\\global_model')\n",
    "print(\"********TRAINING COMPLETED*********** AT {}\".format(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "b0d5016e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49337/49337 [==============================] - 45s 903us/step - loss: 3.8258e-04 - accuracy: 0.9999\n",
      "Test Accuracy of Model_3 (with Batch Normalization): 0.9998752474784851\n"
     ]
    }
   ],
   "source": [
    "global_model.model_3=keras.models.load_model('Himanshu\\models\\global_model')\n",
    "evaluation = global_model.model_3.evaluate(X_test, y_test)\n",
    "print('Test Accuracy of Model_3 (with Batch Normalization): {}'.format(evaluation[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61beb03f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
